{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import model\n",
    "import torch\n",
    "import numpy as np\n",
    "import functools\n",
    "import utils\n",
    "from voc2012_dataset.dataset import VOC2012ClassSegmentation\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get A Test Image\n",
    "\n",
    "Get a test image from voc2012_dataset, by using the **VOC2012ClassSegmentation** class, which is a subclass of **torch.utils.data.Dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "voc_dataset = VOC2012ClassSegmentation('/home/louis/datasets/VOCdevkit/VOC2012')\n",
    "image, gt_bboxes, gt_labels, gt_masks = voc_dataset[5]\n",
    "\n",
    "# convert the numpy arrays to torch.tensors\n",
    "image = torch.tensor(image, device=device, dtype=torch.float64).permute(2,0,1).unsqueeze(0)\n",
    "gt_bboxes = torch.tensor(gt_bboxes, device=device, dtype=torch.float64)\n",
    "gt_labels = torch.tensor(gt_labels, device=device, dtype=torch.float64)\n",
    "gt_masks = torch.tensor(gt_masks, device=device, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get Shared Feature Maps\n",
    "\n",
    "Get feature maps by feeding the image to **FPN**. The output is an array consisted by outputs of different layers of FPN, from p2 to p5, bottom to up(If input image is 224x224, then p2 is 56x56 and p5 is 7x7).\n",
    "\n",
    "The feature maps will be used as input for **RPN**, **ClassificationNetwork** and **MaskNetwork**, this is why it is called **shared**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "resnet = model.ResNet50().to(device)\n",
    "fpn = model.FPN(*resnet.layers(), out_planes=256).to(device)\n",
    "fpn_feature_maps = fpn(image) # we will use fpn_feature_maps a lot in the following code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate Regions by RPN\n",
    "\n",
    "First, get regions by feeding the feature maps to **RPN** network.  The **RPN** generates 3 results:\n",
    "\n",
    "1. rpn_bboxes\n",
    "2. rpn_class_logits\n",
    "3. rpn_class_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rpn = model.RPN(256, 512).to(device)\n",
    "\n",
    "fpn_feature_maps = fpn(image)\n",
    "rpn_regions = []\n",
    "for o in fpn_feature_maps:\n",
    "    rpn_regions.append( rpn(o) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bbox_deltas, class_logits, softmax = zip(*rpn_regions)\n",
    "\n",
    "# flattern the outputs \n",
    "def merge(x, y):\n",
    "    return torch.cat( (x, y), dim=1 )\n",
    "\n",
    "bbox_deltas = functools.reduce(merge, bbox_deltas[1:], bbox_deltas[0])\n",
    "scores = functools.reduce(merge, class_logits[1:], class_logits[0])\n",
    "class_logits = functools.reduce(merge, class_logits[1:], class_logits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get default anchors\n",
    "\n",
    "Now we have bbox_deltas and relative scores from RPN, we should generate anchors and apply the RPN result to them.\n",
    "\n",
    "**Important** make sure everything is generated from bottom to up in the FPN, and the size generated from RPN should be the same as anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scales = [4, 8, 16, 32]\n",
    "ratios = [0.5, 1, 2]\n",
    "feature_strides =[4, 8, 16, 32]\n",
    "input_image_shape = 512\n",
    "anchors = torch.tensor(utils.generate_pyramid_anchors(scales, ratios, input_image_shape, feature_strides), dtype=torch.float64, device=device)\n",
    "# make sure the size is the same! and generate everything from bottom to up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate RoIs.\n",
    "\n",
    "Generate Regions of Interest, the process is: combine the output of `RPN` and anchors we got by `generate_pyramid_anchors` to get the final `fixed sized`, `refined` anchors. The anchors filter process involves the `nms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output: [1, size_of_anchors, 4]\n",
    "rp = model.RegionProposal()\n",
    "rois = rp.forward(bbox_deltas, scores, anchors)\n",
    "\n",
    "# the rois will be regard as \"ground truth\" from now, do not do backpropagation\n",
    "# remove this will also encounter an error when doing backpropagation\n",
    "rois = rois.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate Target\n",
    "Using output of RoI and ground truth bbox, class, mask to generate targets, which will used to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gen_targets = model.GenerateTarget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "generate_rpn_targets = model.GenerateRPNTargets()\n",
    "rpn_class, rpn_bounding_delta = generate_rpn_targets.forward(anchors, gt_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rois, bbox, mask, classes = gen_targets.forward(rois, gt_bboxes, gt_masks, gt_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict!!\n",
    "\n",
    "Now we have all the rois and corresponding ground truth, it's time to do predictions.\n",
    "\n",
    "We'll reuse the feature maps from FPN, predict bbox delta, corresponding class, and mask on each RoI, by using 2 different parallel networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cls_and_reg = model.Classifier(21).to(device)\n",
    "\n",
    "mrcnn_class_logits, mrcnn_probs, mrcnn_bbox = cls_and_reg(fpn_feature_maps, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Mask = model.Mask(21).to(device)\n",
    "preds = Mask(fpn_feature_maps, rois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now it's time to compute loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 4])\n",
      "torch.Size([65280, 4])\n",
      "torch.Size([65280])\n",
      "torch.float64\n",
      "tensor([ 1.,  1.,  1.,  ...,  0.,  0.,  0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rpn_class_loss = model.compute_rpn_class_loss(rpn_class, class_logits)\n",
    "rpn_bbox_loss = model.compute_rpn_bbox_loss(rpn_bounding_delta, bbox_deltas, rpn_class)\n",
    "classes = classes.long()\n",
    "mrcnn_class_loss = model.compute_mrcnn_class_loss(  classes, mrcnn_class_logits )\n",
    "mrcnn_bbox_loss = model.compute_mrcnn_bbox_loss(bbox, classes, mrcnn_bbox)\n",
    "mrcnn_mask_loss = model.compute_mrcnn_mask_loss(mask, classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = rpn_class_loss + rpn_bbox_loss + mrcnn_class_loss + mrcnn_bbox_loss + mrcnn_mask_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
